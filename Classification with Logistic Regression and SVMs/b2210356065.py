# -*- coding: utf-8 -*-
"""BBM409_PA2_notebook.ipynb adlı not defterinin kopyası

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O2fDpZ2eM8dXKzon7yKn7YP5lkTnGZm8

### ## BBM 409 - Programming Assignment 2

* You can add as many cells as you want in-between each question.
* Please add comments to your code to explain your work.  

* Please be careful about the order of runs of cells. Doing the homework, it is likely that you will be running the cells in different orders, however, they will be evaluated in the order they appear. Hence, please try running the cells in this order before submission to make sure they work.    
* Please refer to the homework text for any implementation detail. You should also carefully review the steps explained here.
* This document is also your report. Show your work.

##  Insert personal information (name, surname, student id)

# 1. LOGISTIC REGRESSION TASK (40 points)

### 1. Data Loading and Exploration

##### Download the Bank Marketing dataset from https://drive.google.com/file/d/1t6QAtqfYLMhvv_XUnG4D_UsJcSwgF4an/view?usp=sharing  import other necessary libraries
"""

import pandas
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

!pip install gdown # Install the gdown library if not already installed
import gdown # Import the gdown library
import pandas as pd # Import pandas for data manipulation


file_id = '1t6QAtqfYLMhvv_XUnG4D_UsJcSwgF4an'  # Extract the file ID from the shared link

# Generate the download URL
url = f'https://drive.google.com/uc?id={file_id}'

# Download the file to a temporary location
output_file = 'downloaded_data.csv' # You can change this to your preferred filename
gdown.download(url, output_file, quiet=False)

# Read the downloaded CSV into a pandas DataFrame
df = pd.read_csv(output_file, encoding="utf-8")  # Assuming it's a CSV file

df #to display the DataFrame

"""### 2. calculate correlation between target variable 'y' and other features (5 points)"""

correlation_matrix = df.corr()

correlation_with_y = correlation_matrix['y'] # calculate correlation between target variable 'y'

"""# 1.1 Implementing Logistic Regression with most correlated 2 features

###  Choose the two most correlated features with target feature 'y'
"""

# X=df[[feature1, feature2]]
#y=df['y']
correlation_with_y.sort_values(ascending=False) #duration , poutcome

X = df[['duration','poutcome']] #Most correlated 2 features with y
y = df['y']-1 # Convert y values to sigmoid class values
y.tail()

"""###  * Define your logistic regression model as class without using any built-in libraries
### * Define necessary functions such as sigmoid, fit, predict  (10 points)
"""

class LogisticRegression: # Class for logistic regression

    def __init__(self, learning_rate=0.01, epochs=1000): #Initializer
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.weights = None
        self.bias = None

    def fit(self, X, y): #Training function with logistic regression
        # Initialize weights with zeros and bias with 0
        self.weights = np.zeros(X[1].shape , dtype=np.float64)  # Initialize with zeros, not None
        self.bias = 0

        for _ in range(self.epochs):
            weight_crr = np.zeros(X[1].shape, dtype=np.float64)  # Initialize weight changes
            bias_crr = 0  # Initialize bias change

            for i in range(len(X)):

                prediction = self.sigmoid(np.dot(X[i], self.weights) + self.bias) # Models class prediction
                error = prediction - y[i]  # Formula's error
                # Compute gradients
                weight_crr += error * X[i] # Gradient descent for w
                bias_crr += error #Gradient descent for b

            # Update weights and bias
            self.weights -= self.learning_rate * weight_crr / len(X) # Update weight with avarage of indices
            self.bias -= self.learning_rate * bias_crr / len(X) # Update bias with avarage of indices
        return self


    def sigmoid(self, z): # Activation function
        return 1 / (1 + np.exp(-z))

    def predict(self, X):

      prediction = np.dot(X, self.weights) + self.bias # Predict value
      prediction = np.array(prediction) # Convert type

      # Apply activation function

      prediction[prediction >= 0.5] = 1
      prediction[prediction < 0.5] = 0
      # Returns np array
      return prediction

model = LogisticRegression() # Create object for model

"""Split the dataset into a training set and a validation set (80% training and 20% validation)."""

# Become np array
X = np.array(X, dtype=np.float64)
y = np.array(y, dtype=np.float64)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # split

"""Scale the features using StandardScaler"""

# Create an instance of the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the training data and transform it
X_train = scaler.fit_transform(X_train)

# Transform the test data using the fitted scaler
X_test = scaler.transform(X_test)

"""* Initialize and train the custom logistic regression model"""

model.fit(X_train, y_train) # Train the model

"""* Make predictions on the validation set"""

y_pred = model.predict(X_test) # Make predictions

"""### Evaluate the model's performance, print classification report and confusion matrix  (5 points)"""

cm = confusion_matrix(y_test, y_pred)
#Confusion Matrix
print(cm)

report = classification_report(y_test, y_pred)
# Classification Matrix
print(report)

"""### Print decision boundaries as in PA1 (5 points)"""

def plt_decision_boundary(X, y, model):  # Illustrate decision boundary from a given model
    # Define the range of the grid based on the feature values in X
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1  # Range for the first feature
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1  # Range for the second feature

    # Create a mesh grid for the feature space
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                         np.arange(y_min, y_max, 0.1))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])  # Predict using model for grid points
    Z = np.array(Z).reshape(xx.shape)  # Reshape predictions to match grid shape

    # Plot the decision boundary using contour plot
    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)

    # Scatter plot for the actual data points
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o',
                s=100, cmap=plt.cm.RdYlBu)

    # Add plot details
    plt.title('Decision Boundary for Logistic Regression')  # Title of the plot
    plt.xlabel('Feature 1')  # Label for x-axis
    plt.ylabel('Feature 2')  # Label for y-axis
    plt.show()  # Display the plot

    return xx.ravel(), yy.ravel()  # Return the flattened grid coordinates

# Print decision boundaries as in PA1
dec_boundry_x, dec_boundry_y = plt_decision_boundary(X_test, y_test, model) # Decision boundries values assigned to two arrays that I utilize in next blocks

"""# 1.2 Implementing Logistic Regression using all features.

* Redefine input and target variables. In this experiment, you will use all input features in the dataset.
"""

y = df[['y']]-1 # Convert y values to sigmoid class values
X = df.drop('y', axis=1) # All features without y
X = np.array(X, dtype=np.float64)
y = np.array(y, dtype=np.float64)# Change type

"""* Split the dataset into a training set and a validation set (80% training and 20% validation)."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # split

"""* Scale the features using StandardScaler"""

# Create an instance of the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the training data and transform it
X_train = scaler.fit_transform(X_train)

# Transform the test data using the fitted scaler
X_test = scaler.transform(X_test)

"""### Initialize and train the custom logistic regression model."""

model2 = LogisticRegression() # Train another model with all features
model2.fit(X_train, y_train)

"""* Make predictions on the validation set"""

y_pred = model2.predict(X_test) # Make predictions

"""### Evaluate the model's performance, print classification report and confusion matrix  (5 points)"""

cm = confusion_matrix(y_test, y_pred)
# Confusion
print(cm)

report = classification_report(y_test, y_pred)
# Classification
print(report)

"""### Print decision boundaries as in PA1. Briefly explain the impact of the number of features on the learning ability of the model. (5 points)

Splitting data with fewer features reduces the dimensionality of the subspace. In such lower-dimensional spaces, the ability to separate data may diminish because certain separable patterns or structures in higher dimensions may not be evident in the reduced subspace.
"""

x_values = dec_boundry_x # Plt functions output
y_values = dec_boundry_y
y

"""### After completing the SVM and logistic regression tasks, the best results of the experiments with the SVM and Logistic regression models will be compared in a table. (5 points)"""

from sklearn.svm import SVC
from sklearn.datasets import make_classification


svm_model = SVC(kernel='linear', random_state=42) # Create SVM model
svm_model.fit(X_train, y_train)

y_pred_svm = svm_model.predict(X_test) # Make predictions

print("SVM - Classification Report")
print(classification_report(y_test, y_pred_svm))

cm = confusion_matrix(y_test, y_pred_svm)
print(cm)

report = classification_report(y_test, y_pred_svm)
print(report)

"""# 2. Support Vector Machine Task  (30 points)

* Define your SVM model using sklearn

## 2.1 implementing svm with grid search cv using all features (10 points)

* Define features and target variable, you will use all features of dataset in this task
"""

X , y # There is no change to former X , y

"""* Split the dataset into a training set and a validation set (80% training and 20% validation)."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""* Scale the features using StandardScaler"""

# Create an instance of the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the training data and transform it
X_train = scaler.fit_transform(X_train)

# Transform the test data using the fitted scaler
X_test = scaler.transform(X_test)

"""#### Implement GridSearchCV  (5 points)"""

from sklearn.model_selection import GridSearchCV

# Define parameter grid for hyperparameter tuning
param_grid = {
    'C': [0.1, 1, 10],           # Regularization parameter
    'kernel': ['linear', 'rbf'], # Kernel types
    'gamma': ['scale', 'auto']   # Kernel coefficient options
}

# Set up GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(
    estimator=SVC(),       # SVC model as the estimator
    param_grid=param_grid, # Hyperparameter grid to search
    scoring='accuracy',    # Metric
    cv=5,                  # 5-fold
    verbose=1,             # Display progress of grid search
    n_jobs=-1
)

# Train the model using the training data
grid_search.fit(X_train, y_train)

# Print best parameters and best score
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

"""* Initialize the SVM classifier"""

# Extract the best model from grid search
svm_best_model = grid_search.best_estimator_

"""* Train the SVM classifier with the best parameters found from grid search



"""

# Train the model (already trained by GridSearchCV, but we can confirm)
svm_best_model.fit(X_train, y_train)

"""* Make predictions on the validation set using the best model

"""

# Make predictions on the validation set
y_pred = svm_best_model.predict(X_test)

"""#### Evaluate the model's performance, print classification report and confusion matrix and best parameters found from GridSearchCV  (5 points)"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Validation Accuracy: {accuracy:.2f}")

# Confusion matrix
print(confusion_matrix(y_test, y_pred))

# Classification report
print(classification_report(y_test, y_pred))

"""## 2.2 implementing svm with most correlated 2 features (10 points)

#### Choose the two most correlated features with target feature 'y'
"""

X = df[['duration','poutcome']] # Values from part 1
y = df['y']-1
y
X = np.array(X, dtype=np.float64)
y = np.array(y, dtype=np.float64)

"""* Split the dataset into a training set and a validation set (80% training and 20% validation)."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""* Scale the features using StandardScaler"""

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""*  Initialize the SVM classifier, assign 'C' and 'kernel' parameters from the best hyperparameters you found from GridSearchCV"""

from sklearn.svm import SVC

def svm_grid_search(X_train, y_train):
  grid_search.fit(X_train, y_train)

  # Get the best parameters from GridSearchCV
  best_params = grid_search.best_params_

  # Initialize SVM classifier with the best parameters
  svm_best_model = SVC(C=best_params['C'], kernel=best_params['kernel'])

  # If the kernel is 'poly' or 'rbf', you can add additional parameters like gamma and degree
  if best_params['kernel'] == 'poly':
      svm_best_model = SVC(C=best_params['C'], kernel=best_params['kernel'], degree=best_params['degree'])
  elif best_params['kernel'] == 'rbf':
      svm_best_model = SVC(C=best_params['C'], kernel=best_params['kernel'], gamma=best_params['gamma'])

  return svm_best_model

# SVM model initialized successfully
svm_best_model = svm_grid_search(X_train, y_train)
print(svm_best_model)

"""* Train the SVM classifier"""

svm_best_model.fit(X_train, y_train)

"""* Make predictions on the validation set"""

y_pred = svm_best_model.predict(X_test)

"""#### Evaluate the model's performance, print classification report and confusion matrix  (5 points)"""

from sklearn.metrics import classification_report, confusion_matrix

print("Classification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

"""##### Visualize decision boundary and support vectors (5 points)"""

plt_decision_boundary(X_train, y_train, svm_best_model)

"""## 2.3 implementing svm with least correlated 2 features (10 points)

#### Choose the two least correlated features with target feature 'y'
"""

X = df[['month','job']] # Least correlated features with y
y = df['y']-1
y
X = np.array(X, dtype=np.float64)
y = np.array(y, dtype=np.float64)

"""* Split the dataset into a training set and a validation set (80% training and 20% validation)."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""* Scale the features using StandardScaler"""

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""*  Initialize the SVM classifier, assign 'C' and 'kernel' parameters from the best hyperparameters you found from GridSearchCV"""

svm_best_model = svm_grid_search(X_train, y_train) # Grid search function

print(svm_best_model)

"""* Train the SVM classifier"""

svm_best_model.fit(X_train, y_train)

"""* Make predictions on the validation set"""

y_pred = svm_best_model.predict(X_test)

"""#### Evaluate the model's performance, print classification report and confusion matrix  (5 points)"""

print(classification_report(y_test, y_pred))

print(confusion_matrix(y_test, y_pred))

"""##### Visualize decision boundary and support vectors(5 points)"""

plt_decision_boundary(X_train, y_train, svm_best_model)

"""# 3. Decision Tree Task (30 points)

* Define your decision tree model using sklearn. Also you should define other necessary modules for visualize the decision tree

### Download the dataset from https://drive.google.com/file/d/1D3peA-TzIqJqZDDKTlK0GQ7Ya6FIemFv/view?usp=sharing

### import other necessary libraries
"""

file_id = '1D3peA-TzIqJqZDDKTlK0GQ7Ya6FIemFv'

# Generate the download URL
url = f'https://drive.google.com/uc?id={file_id}'

# Download the file to a temporary location
output_file = 'downloaded_data.csv' # or any preferred name
gdown.download(url, output_file, quiet=False)

# Read the downloaded CSV into a pandas DataFrame
import pandas as pd # Import pandas for data manipulation
df_t = pd.read_csv(output_file, encoding="utf-8")

df_t.head()

"""* Define features and target variable, you will use all features of dataset in this task"""

y_features = df[['y']]-1
X_features = df.drop('y', axis=1)
X = np.array(X_features, dtype=np.float64)
y = np.array(y_features, dtype=np.float64)

"""* Split the dataset into a training set and a validation set (80% training and 20% validation)."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""* Initialize the Decision Tree classifier"""

from sklearn.tree import DecisionTreeClassifier
dt_model = DecisionTreeClassifier(random_state=42)

"""* Train the Decision Tree classifier"""

dt_model.fit(X_train, y_train)

"""* Make predictions on the validation set"""

y_pred = dt_model.predict(X_test)

"""#### Evaluate the model's performance, print classification report and confusion matrix  (10 points)"""

print("Classification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

"""#### Visualize the Decision Tree, show clearly class number, gini value etc.  (10 points)

"""

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree

# Load the dataset
data = load_iris()
X, y = data.data, data.target

# Create and train the decision tree classifier
clf = DecisionTreeClassifier(max_depth=10, random_state=42)  # Limit the depth for clarity
clf.fit(X, y)

# Visualize the decision tree
plt.figure(figsize=(12, 8))  # Set the figure size
plot_tree(clf,
          feature_names=data.feature_names,  # Display feature names
          class_names=data.target_names,    # Display target class names
          filled=True,                      # Fill nodes with colors based on class proportions
          rounded=True,                     # Use rounded node shapes
          impurity=True)                    # Show Gini impurity values in the nodes
plt.show()

"""### Explain briefly the question. What is the role of gini in decision tree? (10 points)

The Gini index is a metric used in decision trees to measure the degree of impurity or disorder in a dataset. It helps the tree decide the best split at each node by evaluating how well a split separates the data into distinct classes.
"""

